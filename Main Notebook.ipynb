{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from parseData import readHeroes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHeroUsage(Y,N):\n",
    "    heroes = Y['hero_picked'].unique()\n",
    "    counts = []\n",
    "    for hero in heroes:\n",
    "        counts.append(len(Y[Y['hero_picked'] == hero]))\n",
    "    counts = np.array(counts)\n",
    "    heroes = heroes[counts.argsort()[-N:]]\n",
    "    counts = counts[counts.argsort()[-N:]]\n",
    "    print(counts[-1]/len(Y))\n",
    "    y_pos = np.arange(len(heroes))\n",
    "    plt.bar(y_pos,counts, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos,heroes)\n",
    "    plt.ylabel('counts')\n",
    "    plt.title('Hero usage')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toHeroID(Y):\n",
    "    hero_ids = []\n",
    "    for y in Y:\n",
    "        if np.argwhere(y==1).size <= 0:\n",
    "            hero_ids.append(0)\n",
    "        else:\n",
    "            hero_ids.append(np.argwhere(y==1)[0][0] +1)\n",
    "    return np.array(hero_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('224051329_22_all_X.csv')\n",
    "Y = pd.read_csv('224051329_22_all_Y.csv')\n",
    "skip_ban = False\n",
    "if ('banned_hero1'not in data.columns):\n",
    "    skip_ban = True\n",
    "heroes =  readHeroes()\n",
    "num_heroes = int(list(heroes.keys())[-1])\n",
    "data =  data.dropna(1,'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2262686567164179\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHeroUsage(Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['match_id','version','start_time'])\n",
    "X = X.fillna(0)\n",
    "allies = np.zeros((X.shape[0],num_heroes))\n",
    "enemies = np.zeros((X.shape[0],num_heroes))\n",
    "if not skip_ban:\n",
    "    banned = np.zeros((X.shape[0],num_heroes))\n",
    "output = np.zeros((Y.shape[0],num_heroes))\n",
    "for i,row in X.iterrows():\n",
    "    allies[i,int(row['ally_hero1'])-1] = 1\n",
    "    allies[i,int(row['ally_hero2'])-1] = 1\n",
    "    allies[i,int(row['ally_hero3'])-1] = 1\n",
    "    allies[i,int(row['ally_hero4'])-1] = 1\n",
    "    enemies[i,int(row['enemy_team1'])-1] = 1\n",
    "    enemies[i,int(row['enemy_team2'])-1] = 1\n",
    "    enemies[i,int(row['enemy_team3'])-1] = 1\n",
    "    enemies[i,int(row['enemy_team4'])-1] = 1\n",
    "    enemies[i,int(row['enemy_team5'])-1] = 1\n",
    "    if not skip_ban:\n",
    "        if 'banned_hero1' in row:\n",
    "            banned[i,int(row['banned_hero1'])-1] = 1\n",
    "        if 'banned_hero2' in row:\n",
    "            banned[i,int(row['banned_hero2'])-1] = 1\n",
    "        if 'banned_hero3' in row:\n",
    "            banned[i,int(row['banned_hero3'])-1] = 1\n",
    "        if 'banned_hero4' in row:\n",
    "            banned[i,int(row['banned_hero4'])-1] = 1\n",
    "        if 'banned_hero5' in row:\n",
    "            banned[i,int(row['banned_hero5'])-1] = 1\n",
    "        if 'banned_hero6' in row:\n",
    "            banned[i,int(row['banned_hero6'])-1] = 1\n",
    "        if 'banned_hero7' in row:\n",
    "            banned[i,int(row['banned_hero7'])-1] = 1\n",
    "        if 'banned_hero8' in row:\n",
    "            banned[i,int(row['banned_hero8'])-1] = 1\n",
    "        if 'banned_hero9' in row:\n",
    "            banned[i,int(row['banned_hero9'])-1] = 1\n",
    "        if 'banned_hero10' in row:\n",
    "            banned[i,int(row['banned_hero10'])-1] = 1\n",
    "        if 'banned_hero11' in row:\n",
    "            banned[i,int(row['banned_hero11'])-1] = 1\n",
    "        if 'banned_hero12' in row:\n",
    "            banned[i,int(row['banned_hero12'])-1] = 1\n",
    "    hero_picked = Y['hero_picked'][i]\n",
    "    hero_picked = Y['hero_picked'][i]\n",
    "    output[i,hero_picked-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "rf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20763723150357996"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rf.predict(X_test)\n",
    "ground_truth = np.array(y_test).flatten()\n",
    "np.sum(prediction == ground_truth) / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "svm_model = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24105011933174225"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = svm_model.predict(X_test)\n",
    "ground_truth = np.array(y_test).flatten()\n",
    "np.sum(prediction == ground_truth) / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_ban:\n",
    "    X = np.concatenate((allies,enemies,banned),1)\n",
    "else:\n",
    "    X = np.concatenate((allies,enemies),1)\n",
    "    \n",
    "Y = output\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100000], Loss: 4.8782\n",
      "Epoch [10/100000], Loss: 4.8590\n",
      "Epoch [15/100000], Loss: 4.8406\n",
      "Epoch [20/100000], Loss: 4.8215\n",
      "Epoch [25/100000], Loss: 4.8015\n",
      "Epoch [30/100000], Loss: 4.7802\n",
      "Epoch [35/100000], Loss: 4.7575\n",
      "Epoch [40/100000], Loss: 4.7333\n",
      "Epoch [45/100000], Loss: 4.7076\n",
      "Epoch [50/100000], Loss: 4.6804\n",
      "Epoch [55/100000], Loss: 4.6516\n",
      "Epoch [60/100000], Loss: 4.6210\n",
      "Epoch [65/100000], Loss: 4.5888\n",
      "Epoch [70/100000], Loss: 4.5549\n",
      "Epoch [75/100000], Loss: 4.5192\n",
      "Epoch [80/100000], Loss: 4.4818\n",
      "Epoch [85/100000], Loss: 4.4428\n",
      "Epoch [90/100000], Loss: 4.4022\n",
      "Epoch [95/100000], Loss: 4.3601\n",
      "Epoch [100/100000], Loss: 4.3166\n",
      "Epoch [105/100000], Loss: 4.2717\n",
      "Epoch [110/100000], Loss: 4.2255\n",
      "Epoch [115/100000], Loss: 4.1781\n",
      "Epoch [120/100000], Loss: 4.1294\n",
      "Epoch [125/100000], Loss: 4.0794\n",
      "Epoch [130/100000], Loss: 4.0282\n",
      "Epoch [135/100000], Loss: 3.9753\n",
      "Epoch [140/100000], Loss: 3.9210\n",
      "Epoch [145/100000], Loss: 3.8653\n",
      "Epoch [150/100000], Loss: 3.8083\n",
      "Epoch [155/100000], Loss: 3.7507\n",
      "Epoch [160/100000], Loss: 3.6931\n",
      "Epoch [165/100000], Loss: 3.6361\n",
      "Epoch [170/100000], Loss: 3.5806\n",
      "Epoch [175/100000], Loss: 3.5275\n",
      "Epoch [180/100000], Loss: 3.4776\n",
      "Epoch [185/100000], Loss: 3.4316\n",
      "Epoch [190/100000], Loss: 3.3898\n",
      "Epoch [195/100000], Loss: 3.3522\n",
      "Epoch [200/100000], Loss: 3.3188\n",
      "Epoch [205/100000], Loss: 3.2889\n",
      "Epoch [210/100000], Loss: 3.2623\n",
      "Epoch [215/100000], Loss: 3.2382\n",
      "Epoch [220/100000], Loss: 3.2161\n",
      "Epoch [225/100000], Loss: 3.1958\n",
      "Epoch [230/100000], Loss: 3.1769\n",
      "Epoch [235/100000], Loss: 3.1592\n",
      "Epoch [240/100000], Loss: 3.1426\n",
      "Epoch [245/100000], Loss: 3.1268\n",
      "Epoch [250/100000], Loss: 3.1119\n",
      "Epoch [255/100000], Loss: 3.0975\n",
      "Epoch [260/100000], Loss: 3.0838\n",
      "Epoch [265/100000], Loss: 3.0705\n",
      "Epoch [270/100000], Loss: 3.0575\n",
      "Epoch [275/100000], Loss: 3.0449\n",
      "Epoch [280/100000], Loss: 3.0326\n",
      "Epoch [285/100000], Loss: 3.0205\n",
      "Epoch [290/100000], Loss: 3.0086\n",
      "Epoch [295/100000], Loss: 2.9969\n",
      "Epoch [300/100000], Loss: 2.9853\n",
      "Epoch [305/100000], Loss: 2.9739\n",
      "Epoch [310/100000], Loss: 2.9626\n",
      "Epoch [315/100000], Loss: 2.9514\n",
      "Epoch [320/100000], Loss: 2.9403\n",
      "Epoch [325/100000], Loss: 2.9293\n",
      "Epoch [330/100000], Loss: 2.9185\n",
      "Epoch [335/100000], Loss: 2.9077\n",
      "Epoch [340/100000], Loss: 2.8970\n",
      "Epoch [345/100000], Loss: 2.8865\n",
      "Epoch [350/100000], Loss: 2.8760\n",
      "Epoch [355/100000], Loss: 2.8656\n",
      "Epoch [360/100000], Loss: 2.8552\n",
      "Epoch [365/100000], Loss: 2.8450\n",
      "Epoch [370/100000], Loss: 2.8349\n",
      "Epoch [375/100000], Loss: 2.8249\n",
      "Epoch [380/100000], Loss: 2.8150\n",
      "Epoch [385/100000], Loss: 2.8051\n",
      "Epoch [390/100000], Loss: 2.7954\n",
      "Epoch [395/100000], Loss: 2.7858\n",
      "Epoch [400/100000], Loss: 2.7763\n",
      "Epoch [405/100000], Loss: 2.7669\n",
      "Epoch [410/100000], Loss: 2.7576\n",
      "Epoch [415/100000], Loss: 2.7484\n",
      "Epoch [420/100000], Loss: 2.7393\n",
      "Epoch [425/100000], Loss: 2.7302\n",
      "Epoch [430/100000], Loss: 2.7213\n",
      "Epoch [435/100000], Loss: 2.7124\n",
      "Epoch [440/100000], Loss: 2.7036\n",
      "Epoch [445/100000], Loss: 2.6948\n",
      "Epoch [450/100000], Loss: 2.6862\n",
      "Epoch [455/100000], Loss: 2.6776\n",
      "Epoch [460/100000], Loss: 2.6691\n",
      "Epoch [465/100000], Loss: 2.6606\n",
      "Epoch [470/100000], Loss: 2.6523\n",
      "Epoch [475/100000], Loss: 2.6441\n",
      "Epoch [480/100000], Loss: 2.6359\n",
      "Epoch [485/100000], Loss: 2.6278\n",
      "Epoch [490/100000], Loss: 2.6199\n",
      "Epoch [495/100000], Loss: 2.6120\n",
      "Epoch [500/100000], Loss: 2.6041\n",
      "Epoch [505/100000], Loss: 2.5964\n",
      "Epoch [510/100000], Loss: 2.5887\n",
      "Epoch [515/100000], Loss: 2.5811\n",
      "Epoch [520/100000], Loss: 2.5736\n",
      "Epoch [525/100000], Loss: 2.5661\n",
      "Epoch [530/100000], Loss: 2.5587\n",
      "Epoch [535/100000], Loss: 2.5514\n",
      "Epoch [540/100000], Loss: 2.5442\n",
      "Epoch [545/100000], Loss: 2.5369\n",
      "Epoch [550/100000], Loss: 2.5298\n",
      "Epoch [555/100000], Loss: 2.5228\n",
      "Epoch [560/100000], Loss: 2.5157\n",
      "Epoch [565/100000], Loss: 2.5088\n",
      "Epoch [570/100000], Loss: 2.5020\n",
      "Epoch [575/100000], Loss: 2.4952\n",
      "Epoch [580/100000], Loss: 2.4885\n",
      "Epoch [585/100000], Loss: 2.4819\n",
      "Epoch [590/100000], Loss: 2.4753\n",
      "Epoch [595/100000], Loss: 2.4689\n",
      "Epoch [600/100000], Loss: 2.4625\n",
      "Epoch [605/100000], Loss: 2.4561\n",
      "Epoch [610/100000], Loss: 2.4499\n",
      "Epoch [615/100000], Loss: 2.4437\n",
      "Epoch [620/100000], Loss: 2.4375\n",
      "Epoch [625/100000], Loss: 2.4314\n",
      "Epoch [630/100000], Loss: 2.4254\n",
      "Epoch [635/100000], Loss: 2.4194\n",
      "Epoch [640/100000], Loss: 2.4135\n",
      "Epoch [645/100000], Loss: 2.4076\n",
      "Epoch [650/100000], Loss: 2.4019\n",
      "Epoch [655/100000], Loss: 2.3961\n",
      "Epoch [660/100000], Loss: 2.3904\n",
      "Epoch [665/100000], Loss: 2.3848\n",
      "Epoch [670/100000], Loss: 2.3792\n",
      "Epoch [675/100000], Loss: 2.3736\n",
      "Epoch [680/100000], Loss: 2.3681\n",
      "Epoch [685/100000], Loss: 2.3627\n",
      "Epoch [690/100000], Loss: 2.3573\n",
      "Epoch [695/100000], Loss: 2.3520\n",
      "Epoch [700/100000], Loss: 2.3467\n",
      "Epoch [705/100000], Loss: 2.3414\n",
      "Epoch [710/100000], Loss: 2.3362\n",
      "Epoch [715/100000], Loss: 2.3311\n",
      "Epoch [720/100000], Loss: 2.3260\n",
      "Epoch [725/100000], Loss: 2.3209\n",
      "Epoch [730/100000], Loss: 2.3159\n",
      "Epoch [735/100000], Loss: 2.3110\n",
      "Epoch [740/100000], Loss: 2.3060\n",
      "Epoch [745/100000], Loss: 2.3012\n",
      "Epoch [750/100000], Loss: 2.2964\n",
      "Epoch [755/100000], Loss: 2.2916\n",
      "Epoch [760/100000], Loss: 2.2869\n",
      "Epoch [765/100000], Loss: 2.2823\n",
      "Epoch [770/100000], Loss: 2.2777\n",
      "Epoch [775/100000], Loss: 2.2731\n",
      "Epoch [780/100000], Loss: 2.2686\n",
      "Epoch [785/100000], Loss: 2.2641\n",
      "Epoch [790/100000], Loss: 2.2597\n",
      "Epoch [795/100000], Loss: 2.2552\n",
      "Epoch [800/100000], Loss: 2.2509\n",
      "Epoch [805/100000], Loss: 2.2465\n",
      "Epoch [810/100000], Loss: 2.2422\n",
      "Epoch [815/100000], Loss: 2.2380\n",
      "Epoch [820/100000], Loss: 2.2338\n",
      "Epoch [825/100000], Loss: 2.2296\n",
      "Epoch [830/100000], Loss: 2.2254\n",
      "Epoch [835/100000], Loss: 2.2213\n",
      "Epoch [840/100000], Loss: 2.2172\n",
      "Epoch [845/100000], Loss: 2.2132\n",
      "Epoch [850/100000], Loss: 2.2092\n",
      "Epoch [855/100000], Loss: 2.2052\n",
      "Epoch [860/100000], Loss: 2.2012\n",
      "Epoch [865/100000], Loss: 2.1973\n",
      "Epoch [870/100000], Loss: 2.1934\n",
      "Epoch [875/100000], Loss: 2.1896\n",
      "Epoch [880/100000], Loss: 2.1857\n",
      "Epoch [885/100000], Loss: 2.1819\n",
      "Epoch [890/100000], Loss: 2.1782\n",
      "Epoch [895/100000], Loss: 2.1744\n",
      "Epoch [900/100000], Loss: 2.1707\n",
      "Epoch [905/100000], Loss: 2.1670\n",
      "Epoch [910/100000], Loss: 2.1634\n",
      "Epoch [915/100000], Loss: 2.1597\n",
      "Epoch [920/100000], Loss: 2.1561\n",
      "Epoch [925/100000], Loss: 2.1526\n",
      "Epoch [930/100000], Loss: 2.1490\n",
      "Epoch [935/100000], Loss: 2.1455\n",
      "Epoch [940/100000], Loss: 2.1419\n",
      "Epoch [945/100000], Loss: 2.1384\n",
      "Epoch [950/100000], Loss: 2.1349\n",
      "Epoch [955/100000], Loss: 2.1315\n",
      "Epoch [960/100000], Loss: 2.1280\n",
      "Epoch [965/100000], Loss: 2.1246\n",
      "Epoch [970/100000], Loss: 2.1213\n",
      "Epoch [975/100000], Loss: 2.1179\n",
      "Epoch [980/100000], Loss: 2.1145\n",
      "Epoch [985/100000], Loss: 2.1111\n",
      "Epoch [990/100000], Loss: 2.1077\n",
      "Epoch [995/100000], Loss: 2.1044\n",
      "Epoch [1000/100000], Loss: 2.1010\n",
      "Epoch [1005/100000], Loss: 2.0976\n",
      "Epoch [1010/100000], Loss: 2.0943\n",
      "Epoch [1015/100000], Loss: 2.0910\n",
      "Epoch [1020/100000], Loss: 2.0877\n",
      "Epoch [1025/100000], Loss: 2.0844\n",
      "Epoch [1030/100000], Loss: 2.0812\n",
      "Epoch [1035/100000], Loss: 2.0780\n",
      "Epoch [1040/100000], Loss: 2.0748\n",
      "Epoch [1045/100000], Loss: 2.0716\n",
      "Epoch [1050/100000], Loss: 2.0684\n",
      "Epoch [1055/100000], Loss: 2.0652\n",
      "Epoch [1060/100000], Loss: 2.0621\n",
      "Epoch [1065/100000], Loss: 2.0590\n",
      "Epoch [1070/100000], Loss: 2.0559\n",
      "Epoch [1075/100000], Loss: 2.0528\n",
      "Epoch [1080/100000], Loss: 2.0497\n",
      "Epoch [1085/100000], Loss: 2.0467\n",
      "Epoch [1090/100000], Loss: 2.0436\n",
      "Epoch [1095/100000], Loss: 2.0406\n",
      "Epoch [1100/100000], Loss: 2.0377\n",
      "Epoch [1105/100000], Loss: 2.0347\n",
      "Epoch [1110/100000], Loss: 2.0317\n",
      "Epoch [1115/100000], Loss: 2.0288\n",
      "Epoch [1120/100000], Loss: 2.0259\n",
      "Epoch [1125/100000], Loss: 2.0229\n",
      "Epoch [1130/100000], Loss: 2.0200\n",
      "Epoch [1135/100000], Loss: 2.0171\n",
      "Epoch [1140/100000], Loss: 2.0142\n",
      "Epoch [1145/100000], Loss: 2.0112\n",
      "Epoch [1150/100000], Loss: 2.0084\n",
      "Epoch [1155/100000], Loss: 2.0055\n",
      "Epoch [1160/100000], Loss: 2.0026\n",
      "Epoch [1165/100000], Loss: 1.9997\n",
      "Epoch [1170/100000], Loss: 1.9969\n",
      "Epoch [1175/100000], Loss: 1.9940\n",
      "Epoch [1180/100000], Loss: 1.9912\n",
      "Epoch [1185/100000], Loss: 1.9885\n",
      "Epoch [1190/100000], Loss: 1.9857\n",
      "Epoch [1195/100000], Loss: 1.9830\n",
      "Epoch [1200/100000], Loss: 1.9803\n",
      "Epoch [1205/100000], Loss: 1.9776\n",
      "Epoch [1210/100000], Loss: 1.9749\n",
      "Epoch [1215/100000], Loss: 1.9722\n",
      "Epoch [1220/100000], Loss: 1.9695\n",
      "Epoch [1225/100000], Loss: 1.9668\n",
      "Epoch [1230/100000], Loss: 1.9642\n",
      "Epoch [1235/100000], Loss: 1.9615\n",
      "Epoch [1240/100000], Loss: 1.9589\n",
      "Epoch [1245/100000], Loss: 1.9563\n",
      "Epoch [1250/100000], Loss: 1.9537\n",
      "Epoch [1255/100000], Loss: 1.9511\n",
      "Epoch [1260/100000], Loss: 1.9485\n",
      "Epoch [1265/100000], Loss: 1.9459\n",
      "Epoch [1270/100000], Loss: 1.9434\n",
      "Epoch [1275/100000], Loss: 1.9408\n",
      "Epoch [1280/100000], Loss: 1.9383\n",
      "Epoch [1285/100000], Loss: 1.9358\n",
      "Epoch [1290/100000], Loss: 1.9333\n",
      "Epoch [1295/100000], Loss: 1.9309\n",
      "Epoch [1300/100000], Loss: 1.9284\n",
      "Epoch [1305/100000], Loss: 1.9260\n",
      "Epoch [1310/100000], Loss: 1.9236\n",
      "Epoch [1315/100000], Loss: 1.9211\n",
      "Epoch [1320/100000], Loss: 1.9188\n",
      "Epoch [1325/100000], Loss: 1.9164\n",
      "Epoch [1330/100000], Loss: 1.9140\n",
      "Epoch [1335/100000], Loss: 1.9117\n",
      "Epoch [1340/100000], Loss: 1.9093\n",
      "Epoch [1345/100000], Loss: 1.9070\n",
      "Epoch [1350/100000], Loss: 1.9046\n",
      "Epoch [1355/100000], Loss: 1.9022\n",
      "Epoch [1360/100000], Loss: 1.8999\n",
      "Epoch [1365/100000], Loss: 1.8976\n",
      "Epoch [1370/100000], Loss: 1.8953\n",
      "Epoch [1375/100000], Loss: 1.8930\n",
      "Epoch [1380/100000], Loss: 1.8907\n",
      "Epoch [1385/100000], Loss: 1.8884\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "actual_ids = toHeroID(y_train)-1\n",
    "actual_ids = actual_ids\n",
    "output_size = y_train.shape[1]\n",
    "# output_size = \n",
    "hidden_size = 5\n",
    "num_epochs = 100000\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "# log_model = nn.Linear(input_size,output_size)\n",
    "# might need a sigmoid ^\n",
    "nn_model = NeuralNet(input_size,hidden_size,output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr = learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(np.array(X_train,dtype=np.float32))\n",
    "    targets = torch.from_numpy(np.array(actual_ids,dtype=np.int64))\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = nn_model(inputs)\n",
    "    loss = criterion(outputs.squeeze(), targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nn_model(torch.from_numpy(np.array(X_test,dtype=np.float32))).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(y_test,axis=1)+1\n",
    "sum((np.argmax(prediction,axis=1)+1) == labels )/ len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "rf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf.predict(X_test)\n",
    "sum((np.argmax(prediction,axis=1)+1) == labels )/ len(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
